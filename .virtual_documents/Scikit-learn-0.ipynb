








!git clone https://github.com/ARIM-Academy/Advanced_Tutorial_1.git
%cd Advanced_Tutorial_1








#ライブラリ
import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')





#データセットの読み込み
df = pd.read_csv('data/Boston.csv', index_col=0)


#データセットの確認
df





#　記述子（特徴量）の選定および削除
df = df.drop(['black'],axis = 1)
df








#概要統計
df.describe()





#　ペアプロットによる各変数間の二次元空間像
sns.pairplot(df)
plt.show()





#　ペアプロットによる各変数間の二次元空間像（その２）
TARGET=['indus','nox','rm','age','dis','rad','lstat']
sns.pairplot(df[TARGET])
plt.show()





#　相関係数
corr = df.corr()
corr





#相関係数をヒートマップ化する

plt.figure(figsize=(10,10))

sns.heatmap(corr, annot=True)
plt.show()








# 住宅価格のヒストグラム
fig, ax = plt.subplots(figsize=(6,6))

#着目する変数の指定（ここでは住宅価格）
X = df['medv']

#ビン数は20分割
ax.hist(X, bins = 20)

#ラベル名
plt.xlabel(df.columns[12], fontsize =14)
plt.ylabel('Freq', fontsize =14)

plt.grid()
plt.show()





#住宅価格と部屋数との相関図
fig, ax = plt.subplots(figsize=(6,6))

#着目する変数の指定（ここでは住宅価格と部屋数）
X = df[['rm']]
y = df[['medv']]

ax.scatter(X,y)

plt.xlabel("Average Number of Rooms", fontsize =14)
plt.ylabel("Housing Price", fontsize =14)

# 横軸のレンジを0から10に設定
plt.xlim(0, 10)
plt.ylim(0, 60)

plt.grid()
plt.show()





#住宅価格と部屋数との相関図

# 50を超える価格のデータを除外
filtered_df = df[df['medv'] < 50]

# フィルタリングされたデータの取得
X = filtered_df[['rm']]
y = filtered_df[['medv']]

fig, ax = plt.subplots(figsize=(6,6))

ax.scatter(X,y)

plt.xlabel("Average Number of Rooms", fontsize =14)
plt.ylabel("Housing Price", fontsize =14)

# 横軸のレンジを0から10に設定
plt.xlim(0, 10)
plt.ylim(0, 60)

plt.grid()
plt.show()








# 線形回帰モデル
from sklearn.linear_model import LinearRegression





# フィルタリングされたデータの取得
X = filtered_df[['rm']]
y = filtered_df[['medv']]





#予測器の作成
model = LinearRegression()

#モデルの訓練（フィッティング）
model.fit(X, y)





y_pred = model.predict(X)





#住宅価格と部屋数との関係図
fig, ax = plt.subplots(figsize=(6,6))

ax.scatter(X, y)
ax.plot(X, y_pred, linestyle="solid", color = "red")

plt.xlabel("Average Number of Rooms", fontsize =14)
plt.ylabel("Housing Price", fontsize =14)

# 横軸のレンジを0から10に設定
plt.xlim(0, 10)
plt.ylim(0, 60)

plt.grid()
plt.show()





print('①　係数：', model.coef_[0])
print('②　切片：',model.intercept_)
print('③　決定係数 R^2： {:.3f}'.format(model.score(X, y)))

# 相関係数を決定係数から計算
r_squared = model.score(X, y)
correlation_coefficient = np.sqrt(r_squared)
print('④　相関係数： {:.3f}'.format(correlation_coefficient))











#住宅価格（medv）が50以上のものを排除

df = df[df['medv'] < 50]

#住宅価格（medv）を目的変数として、それ以外を説明変数とする
X = df.iloc[0:, 0:12] # 説明変数行列
y = df.iloc[0:, -1]   # 目的変数





df





#データセットの分割のライブラリの読み込み
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size = 0.2,   # CV用データ分割
                                                    random_state=42
                                                    )  





X_train











# 線形回帰モデル
from sklearn.linear_model import LinearRegression

#予測器の作成
model = LinearRegression()
model.fit(X_train, y_train)





#係数（重み）と切片
print('係数：',model.coef_)
print('切片：',model.intercept_)


#係数をPandasのSeries型に変換
coef = pd.Series(model.coef_, index = X.columns)
coef





# 回帰係数の可視化
fig, ax = plt.subplots(figsize=(6,6))

imp_coef = coef.sort_values()

ax.barh(X.columns,
        imp_coef,
        color ='blue')

ax.grid()
plt.title('Regression coefficient') 

plt.show() 





#予測値
y_pred_train = model.predict(X_train)


# 訓練データの精度
r2_score = model.score(X_train, y_train)
print('訓練データ決定係数:{:.3f}'.format(r2_score))





#テストデータの予測値
y_pred_test = model.predict(X_test)


# テストデータの精度
r2_score = model.score(X_test, y_test)
print('テストデータ決定係数:{:.3f}'.format(r2_score))





fig, ax = plt.subplots(figsize=(6,6))

#データのプロット
ax.plot(y_train, y_pred_train, '.', c = 'blue', label ='train')
ax.plot(y_test, y_pred_test, '.', c = 'red', label ='test')

# 対角線のプロット
ax.plot([0, 100], [0, 100], 
        linestyle='--', 
        color ='gray')  

#グラフの修飾
ax.axis('square')
ax.set_xlabel('Observed',fontsize = 14)
ax.set_ylabel('Predicted',fontsize = 14)

ax.set_xlim(0,60)
ax.set_ylim(0,60)
ax.legend(fontsize = 14)
ax.grid()

plt.show()








# Lasso回帰モデル
from sklearn.linear_model import Lasso

#予測器の作成
model = Lasso()
model.fit(X_train, y_train)





#係数（重み）と切片
print('係数：',model.coef_)
print('切片：',model.intercept_)


coef = pd.Series(model.coef_, index = X.columns)
coef


# 回帰係数の可視化
fig, ax = plt.subplots(figsize=(6,6))

imp_coef = coef.sort_values()

ax.barh(X.columns,
        imp_coef,
        color ='blue')

ax.grid()
plt.title('Lasso coefficient') 

plt.show() 





#予測値
y_pred_train = model.predict(X_train)


# 訓練データの精度
r2_score = model.score(X_train, y_train)
print('訓練データ決定係数:{:.3f}'.format(r2_score))





#テストデータの予測値
y_pred_test = model.predict(X_test)


# 訓練データの精度
r2_score = model.score(X_test, y_test)
print('テストデータ決定係数:{:.3f}'.format(r2_score))





fig, ax = plt.subplots(figsize=(6,6))

#データのプロット
ax.plot(y_train, y_pred_train, '.', c = 'blue', label ='train')
ax.plot(y_test, y_pred_test, '.', c = 'red', label ='test')

# 対角線のプロット
ax.plot([0, 100], [0, 100], 
        linestyle='--', 
        color ='gray')  

#グラフの修飾
ax.axis('square')
ax.set_xlabel('Observed',fontsize = 14)
ax.set_ylabel('Predicted',fontsize = 14)

ax.set_xlim(0,60)
ax.set_ylim(0,60)
ax.legend(fontsize = 14)
ax.grid()

plt.show()








#ランダムフォレスト回帰
from sklearn.ensemble import RandomForestRegressor

#予測器の作成
model = RandomForestRegressor(n_estimators=10)
model.fit(X_train, y_train)





coef = pd.Series(model.feature_importances_, index = X.columns)
coef


# 回帰係数の可視化
fig, ax = plt.subplots(figsize=(6,6))

imp_coef = coef.sort_values()

ax.barh(X.columns,
        imp_coef,
        color ='blue')

ax.grid()
plt.title('Random Forrest Feature Importance') 
plt.show()





#予測値
y_pred_train = model.predict(X_train)


# 訓練データの精度
r2_score = model.score(X_train, y_train)
print('訓練データ決定係数:{:.3f}'.format(r2_score))





#テストデータの予測値
y_pred_test = model.predict(X_test)


# 訓練データの精度
r2_score = model.score(X_test, y_test)
print('テストデータ決定係数:{:.3f}'.format(r2_score))





fig, ax = plt.subplots(figsize=(6,6))

#データのプロット
ax.plot(y_train, y_pred_train, '.', c = 'blue', label ='train')
ax.plot(y_test, y_pred_test, '.', c = 'red', label ='test')

# 対角線のプロット
ax.plot([0, 100], [0, 100], 
        linestyle='--', 
        color ='gray')  

#グラフの修飾
ax.axis('square')
ax.set_xlabel('Observed',fontsize = 14)
ax.set_ylabel('Predicted',fontsize = 14)

ax.set_xlim(0,60)
ax.set_ylim(0,60)
ax.legend(fontsize = 14)
ax.grid()

plt.show()









